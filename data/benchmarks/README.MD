# Data Structure Overview

data/
â”œâ”€â”€ benchmarks/
â”‚   â”œâ”€â”€ fintabnet/                     # Financial table benchmark
â”‚   â”‚   â”œâ”€â”€ raw_parquet/               # Source parquet files (NOT committed)
â”‚   â”‚   â”œâ”€â”€ pdf/                       # Synthesized single-page PDFs
â”‚   â”‚   â””â”€â”€ gt_json/                   # OCR-style ground truth (generated)
â”‚   â”‚
â”‚   â”œâ”€â”€ doclaynet/                     # Complex document layout benchmark
â”‚   â”‚   â”œâ”€â”€ raw_json/val/              # Source COCO-style annotations
â”‚   â”‚   â”œâ”€â”€ images/                    # Page images (dataset-provided)
â”‚   â”‚   â”œâ”€â”€ pdf/                       # Synthesized single-page PDFs
â”‚   â”‚   â””â”€â”€ gt_json/                   # OCR-style ground truth (generated)
â”‚   â”‚
â”‚   â”œâ”€â”€ icdar2013/                     # Text OCR baseline
â”‚   â”‚   â”œâ”€â”€ pdf/
â”‚   â”‚   â””â”€â”€ gt_json/
â”‚   â”‚
â”‚   â””â”€â”€ README.md                      # Dataset preparation & usage


# ğŸ“Š Benchmark Datasets

## 1. FinTabNet â€“ Financial Table Documents

Source: HuggingFace (FinTabNet validation split)

Original Format:
- Parquet files containing table annotations and rendered page images

Pipeline Format:
- Page images â†’ single-page PDFs (synthesized)
- Table HTML / restored text â†’ OCR-style ground truth JSON

Purpose:
- Evaluate OCR robustness on financial documents containing dense tables
- Measure text extraction accuracy in table-heavy layouts (CER/WER)
## 2. DocLayNet â€“ Complex Document Layouts

Source: Kaggle (DocLayNet validation split)

Original Format:
- Page images + COCO-style layout annotations

Pipeline Format:
- Page images â†’ single-page PDFs (synthesized)
- Layout text regions â†’ OCR-style ground truth JSON

Purpose:
- Evaluate OCR performance on complex multi-region documents
- Stress-test layout-heavy financial and report-style pages
## 3. ICDAR 2013 â€“ Text OCR Baseline

Source: Synthetic scaffold aligned with ICDAR 2013 format

Format:
- PDF documents
- Line- and word-level OCR ground truth JSON

Purpose:
- Establish a clean baseline for pure text OCR
- Validate metric correctness independent of layout complexity

# ğŸ› ï¸ Scripts Documentation

# Data Processing Scripts

convert_fintabnet.py

Purpose: Convert FinTabNet parquet files to JSON format
Input: data/benchmarks/fintabnet/raw_parquet/*.parquet
Output: data/benchmarks/fintabnet/gt_json/FinTabNet_OTSL_val.json
Key Features:

    Reads multiple parquet files

    Combines into single JSON

    Handles numpy serialization issues

    Preserves table structure data (otsl, html, cells)

process_doclaynet.py

Purpose: Process DocLayNet JSON files for benchmarking
Input: data/benchmarks/doclaynet/raw_json/val/*.json
Output: data/benchmarks/doclaynet/gt_json/DocLayNet_val.json
Key Features:

    Loads and combines multiple JSON files

    Validates COCO annotation format

    Extracts layout and region data

    Ensures consistent structure

Validation & Verification Scripts
validate_benchmarks.py

Purpose: Validate all benchmark datasets
Checks:

    JSON syntax validity

    Minimum record counts

    Required field presence

    Data structure consistency

    File size expectations

Usage:

python validate_benchmarks.py

create_icdr_sample.py

Purpose:Creates a synthetic placeholder dataset for ICDAR 2013 benchmark when the actual dataset is not immediately available. Establishes the data format and pipeline structure for pure text OCR validation during the T0.3 planning phase

Core Operations

    Generates Synthetic Document Data - Creates realistic text samples mimicking ICDAR 2013 format

    Establishes JSON Structure - Defines exact format expected by evaluation pipeline

    Sets Up Directory Structure - Creates required folder hierarchy automatically

    Provides Validation Samples - Enables immediate pipeline testing

Output Format

[
  {
    "id": "icdar_001",
    "text": "Full document text for OCR testing",
    "lines": [
      {"text": "Line 1 text", "bbox": [x, y, width, height]},
      {"text": "Line 2 text", "bbox": [x, y, width, height]}
    ],
    "words": [
      {"text": "Word1", "bbox": [x, y, width, height]},
      {"text": "Word2", "bbox": [x, y, width, height]}
    ]
  }
]

# ğŸš€ Quick Start
1. Setup Environment
bash

# Create directory structure
mkdir -p data/benchmarks/{fintabnet,doclaynet,icdar2013}/{raw,gt_json}
mkdir -p data/ground_truth


2. Download Data


 Download FinTabNet
 Download manually from HuggingFace(download the val_*.paquet files) and place them in data/benchmark/fintabnet/gt_json

 Download DocLayNet (from Kaggle)
 Manual download required: val.json â†’ data/benchmarks/doclaynet/raw_json/val/

 Run the python create_icdar_sample.py

3. Process Data


# Convert FinTabNet
python convert_fintabnet.py

# Process DocLayNet
python process_doclaynet.py



4. Validate


# Validate all benchmarks
python validate_benchmarks.py


ğŸ“ˆ Evaluation Metrics
FinTabNet Metrics

    Table Detection Accuracy

    Cell Recognition Accuracy

    HTML Structure Fidelity

DocLayNet Metrics

    Layout Parsing Accuracy

    Region Detection F1 Score

    Bounding Box IoU

ICDAR 2013 Metrics

    Character Recognition Rate (CRR)

    Word Accuracy

    Line Detection Accuracy


