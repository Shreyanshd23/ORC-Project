"""
timer.py

Placeholder for future timing and profiling utilities.
Intended for use in Phase 2 performance instrumentation.
"""
# import time import logging import threading from pathlib import Path from typing import Dict, Any, Optional, Callable from PyPDF2 import PdfReader, PdfWriter from docling.document_converter import DocumentConverter, PdfFormatOption from docling.datamodel.base_models import InputFormat from docling.datamodel.pipeline_options import PdfPipelineOptions # ----------------------------- # Logging # ----------------------------- logging.basicConfig( level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s" ) logger = logging.getLogger(__name__) # ----------------------------- # PDF validation # ----------------------------- def is_valid_pdf(pdf_path: str) -> bool: try: PdfReader(pdf_path) return True except Exception: return False # ----------------------------- # PDF Split Helper # ----------------------------- def create_temp_pdf(original_path: Path, start_page: int, end_page: int) -> Path: reader = PdfReader(str(original_path)) writer = PdfWriter() for i in range(start_page - 1, end_page): writer.add_page(reader.pages[i]) temp_path = original_path.parent / f"__temp_{original_path.name}" with open(temp_path, "wb") as f: writer.write(f) return temp_path # ----------------------------- # Docling OCR Engine # ----------------------------- class OCREngine: def __init__(self): logger.info("Docling OCR Engine initialized") def _create_converter(self): pipeline_options = PdfPipelineOptions() pipeline_options.do_ocr = True pipeline_options.do_table_structure = True pipeline_options.table_structure_options.do_cell_matching = True pipeline_options.enable_remote_services = False return DocumentConverter( allowed_formats=[InputFormat.PDF], format_options={ InputFormat.PDF: PdfFormatOption( pipeline_options=pipeline_options ) } ) # ----------------------------- # Main OCR method # ----------------------------- # ----------------------------- # Logging # ----------------------------- logging.basicConfig( level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s" ) logger = logging.getLogger(__name__) # ----------------------------- # PDF validation # ----------------------------- def is_valid_pdf(pdf_path: str) -> bool: try: PdfReader(pdf_path) return True except Exception: return False def has_embedded_text(pdf_path: str) -> bool: try: reader = PdfReader(pdf_path) for page in reader.pages[:3]: # check first 3 pages if page.extract_text(): return True return False except Exception: return False # ----------------------------- # PDF Split Helper # ----------------------------- def create_temp_pdf(original_path: Path, start_page: int, end_page: int) -> Path: reader = PdfReader(str(original_path)) writer = PdfWriter() for i in range(start_page - 1, end_page): writer.add_page(reader.pages[i]) temp_path = original_path.parent / f"__temp_{original_path.name}" with open(temp_path, "wb") as f: writer.write(f) return temp_path # ----------------------------- # Docling OCR Engine # ----------------------------- class OCREngine: def __init__(self): logger.info("Docling OCR Engine initialized") def _clean_text(self, text: str) -> str: import re text = text.encode("ascii", "ignore").decode() text = re.sub(r"\s+", " ", text) return text.strip() def _create_converter(self, force_ocr: bool = False): pipeline_options = PdfPipelineOptions() # Smart OCR decision pipeline_options.do_ocr = force_ocr pipeline_options.table_structure_options.do_cell_matching = True pipeline_options.do_table_structure = True pipeline_options.table_structure_options.do_cell_matching = True pipeline_options.enable_remote_services = False return DocumentConverter( allowed_formats=[InputFormat.PDF], format_options={ InputFormat.PDF: PdfFormatOption( pipeline_options=pipeline_options ) } ) # ----------------------------- # Main OCR method # ----------------------------- def process_pdf( self, pdf_path: str, progress_cb: Optional[Callable[[str], None]] = None, start_page: Optional[int] = None, end_page: Optional[int] = None ) -> Dict[str, Any]: pdf_path = Path(pdf_path).expanduser().resolve() if not pdf_path.exists(): raise FileNotFoundError(f"PDF not found: {pdf_path}") if not is_valid_pdf(str(pdf_path)): return { "success": False, "error": "Invalid or corrupted PDF", "pdf": str(pdf_path) } def notify(msg: str): if progress_cb: progress_cb(msg) logger.info(msg) try: reader = PdfReader(str(pdf_path)) total_pages = len(reader.pages) notify(f"PDF loaded successfully ({total_pages} pages detected)") # ----------------------------- # Split PDF if page range given # ----------------------------- temp_pdf = pdf_path if start_page and end_page: if start_page < 1 or end_page > total_pages: raise ValueError("Invalid page range") notify(f"Processing pages {start_page}-{end_page}") temp_pdf = create_temp_pdf(pdf_path, start_page, end_page) notify("OCR started ‚Äî processing pages") # ----------------------------- # Progress simulation # ----------------------------- progress = {"page": 0} stop_flag = {"done": False} pages_to_process = ( end_page - start_page + 1 if start_page and end_page else total_pages ) def progress_simulator(): while not stop_flag["done"] and progress["page"] < pages_to_process: time.sleep(1.5) progress["page"] += 1 notify(f"Processing page {progress['page']} of {pages_to_process}") t = threading.Thread(target=progress_simulator, daemon=True) t.start() # ----------------------------- # Run Docling # ----------------------------- start_time = time.time() # Detect if PDF already has text embedded = has_embedded_text(str(temp_pdf)) notify(f"Embedded text detected: {embedded}") converter = self._create_converter(force_ocr=not embedded) result = converter.convert(str(temp_pdf)) elapsed = round(time.time() - start_time, 2) stop_flag["done"] = True t.join() # Clean up temp file if temp_pdf != pdf_path and temp_pdf.exists(): temp_pdf.unlink() notify("OCR completed successfully") raw_text = result.document.export_to_text() cleaned_text = self._clean_text(raw_text) return { "success": True, "text": cleaned_text, "markdown": result.document.export_to_markdown(), "pages": pages_to_process, "time_sec": elapsed } except Exception as e: logger.exception("OCR processing failed") return { "success": False, "error": str(e), "pdf": str(pdf_path) } import re from typing import Any, Dict, List # ============================================================ # Normalization # ============================================================ def normalize_text(text: str) -> str: return " ".join(text.lower().split()) def normalize_number(n: str) -> str: return re.sub(r"[^\d\.-]", "", n) # ============================================================ # Recursive Extraction # ============================================================ def extract_all_numbers(obj: Any) -> List[str]: numbers = [] if isinstance(obj, dict): for v in obj.values(): numbers.extend(extract_all_numbers(v)) elif isinstance(obj, list): for item in obj: numbers.extend(extract_all_numbers(item)) elif isinstance(obj, str): found = re.findall(r"-?\d[\d,\.]*", obj) numbers.extend(found) elif isinstance(obj, (int, float)): numbers.append(str(obj)) return numbers def extract_all_text(obj: Any) -> List[str]: texts = [] if isinstance(obj, dict): for v in obj.values(): texts.extend(extract_all_text(v)) elif isinstance(obj, list): for item in obj: texts.extend(extract_all_text(item)) elif isinstance(obj, str): t = obj.strip() if t: texts.append(t) return texts # ============================================================ # Financial Accuracy # ============================================================ def financial_accuracy_report(gt_json: Dict[str, Any], ocr_text: str) -> Dict[str, Any]: normalized_ocr = normalize_text(ocr_text) normalized_ocr_numbers = normalize_number(ocr_text) # -------------------------------------------------------- # Numeric Accuracy # -------------------------------------------------------- gt_numbers = extract_all_numbers(gt_json) gt_numbers = [normalize_number(n) for n in gt_numbers if normalize_number(n)] total_numbers = len(gt_numbers) matched_numbers = sum( 1 for n in gt_numbers if n and n in normalized_ocr_numbers ) numeric_accuracy = matched_numbers / total_numbers if total_numbers else 0 # -------------------------------------------------------- # Line Item Accuracy # -------------------------------------------------------- gt_texts = extract_all_text(gt_json) gt_texts = [normalize_text(t) for t in gt_texts if len(t.strip()) > 3] total_items = len(gt_texts) matched_items = sum( 1 for t in gt_texts if t in normalized_ocr ) line_item_accuracy = matched_items / total_items if total_items else 0 # -------------------------------------------------------- # Section Accuracy # -------------------------------------------------------- required_sections = [ "balance sheet", "statement of profit and loss", "cash flow", "auditor" ] matched_sections = sum( 1 for sec in required_sections if sec in normalized_ocr ) section_accuracy = matched_sections / len(required_sections) # -------------------------------------------------------- # Disclosure Accuracy # -------------------------------------------------------- disclosure_keywords = [ "true and fair", "for the year ended", "earnings per equity share", "as at march" ] matched_disclosures = sum( 1 for d in disclosure_keywords if d in normalized_ocr ) disclosure_accuracy = matched_disclosures / len(disclosure_keywords) # -------------------------------------------------------- # Final Score (Weighted) # -------------------------------------------------------- final_score = ( numeric_accuracy * 0.4 + line_item_accuracy * 0.2 + section_accuracy * 0.2 + disclosure_accuracy * 0.2 ) return { "numeric_accuracy": round(numeric_accuracy, 4), "line_item_accuracy": round(line_item_accuracy, 4), "section_accuracy": round(section_accuracy, 4), "disclosure_accuracy": round(disclosure_accuracy, 4), "financial_overall_score": round(final_score, 4) } import re from typing import Any, Dict, List # ------------------------------------------------ # BASIC HELPERS # ------------------------------------------------ def normalize(text: str) -> str: return " ".join(text.lower().split()) def normalize_number(n: str) -> str: return re.sub(r"[^\d.]", "", n) def extract_numbers(text: str) -> List[str]: return [normalize_number(x) for x in re.findall(r"\d[\d,\.]*", text)] # ------------------------------------------------ # NUMERIC MATCH SCORE # ------------------------------------------------ def numeric_overlap(gt: str, pred: str): gt_nums = extract_numbers(gt) pred_nums = extract_numbers(pred) matched = sum(1 for n in gt_nums if n in pred_nums) total = len(gt_nums) return matched / total if total else 0 # ------------------------------------------------ # TABLE PARSER FROM MARKDOWN # ------------------------------------------------ def parse_markdown_table(md: str): rows = [] for line in md.splitlines(): if "|" in line: cells = [c.strip() for c in line.split("|") if c.strip()] if cells: rows.append(cells) return rows # ------------------------------------------------ # STRUCTURE SCORE # ------------------------------------------------ def structure_score(gt_rows, pred_rows): if not gt_rows or not pred_rows: return 0 return min(len(pred_rows), len(gt_rows)) / max(len(pred_rows), len(gt_rows)) # ------------------------------------------------ # ROW MATCH SCORE # ------------------------------------------------ def row_score(gt_rows, pred_rows): if not gt_rows: return 0 matched = 0 for gt in gt_rows: gt_line = " ".join(gt) for pr in pred_rows: pr_line = " ".join(pr) if normalize(gt_line) in normalize(pr_line): matched += 1 break return matched / len(gt_rows) # ------------------------------------------------ # COLUMN SCORE # ------------------------------------------------ def column_score(gt_rows, pred_rows): if not gt_rows or not pred_rows: return 0 gt_cols = max(len(r) for r in gt_rows) pr_cols = max(len(r) for r in pred_rows) return min(gt_cols, pr_cols) / max(gt_cols, pr_cols) # ====================================================== # AUTO ADAPTIVE FINANCIAL METRIC # ====================================================== def financial_accuracy_report(gt_json: Any, ocr_output: str) -> Dict: # ===================================================== # WORD-LEVEL DATASET # ===================================================== if isinstance(gt_json, list): gt_text = " ".join( item["text"] for item in gt_json if "text" in item ) num_score = numeric_overlap(gt_text, ocr_output) return { "mode": "word_level", "numeric_score": round(num_score,4), "structure_score": None, "row_score": None, "column_score": None, "financial_overall_score": round(num_score,4) } # ===================================================== # STRUCTURED TABLE FORMAT # ===================================================== if isinstance(gt_json, dict) and "tables" in gt_json: table = gt_json["tables"][0] # ---------- HTML FORMAT ---------- if "html" in table: gt_rows = parse_markdown_table(table["html"]) pred_rows = parse_markdown_table(ocr_output) gt_text = table["html"] # ---------- CELL FORMAT ---------- elif "cells" in table: gt_rows = [] current = [] for c in table["cells"]: current.append(str(c.get("value",""))) if c.get("end_row"): gt_rows.append(current) current = [] pred_rows = parse_markdown_table(ocr_output) gt_text = " ".join(sum(gt_rows,[])) else: return {"mode":"unknown_table_format","financial_overall_score":0} # ---------- SCORES ---------- num = numeric_overlap(gt_text, ocr_output) struct = structure_score(gt_rows, pred_rows) row = row_score(gt_rows, pred_rows) col = column_score(gt_rows, pred_rows) final = ( num*0.4 + struct*0.2 + row*0.2 + col*0.2 ) return { "mode":"table_evaluation", "numeric_score": round(num,4), "structure_score": round(struct,4), "row_score": round(row,4), "column_score": round(col,4), "financial_overall_score": round(final,4) } # ===================================================== # ROOT HTML FORMAT # ===================================================== if isinstance(gt_json, dict) and "html" in gt_json: num = numeric_overlap(gt_json["html"], ocr_output) return { "mode":"root_html", "financial_overall_score": round(num,4) } # ===================================================== # UNKNOWN FORMAT # ===================================================== return { "mode":"unsupported_format", "financial_overall_score":0 } from jiwer import cer, wer from rapidfuzz.distance import Levenshtein import re from typing import Dict # ----------------------------- # Normalization # ----------------------------- def normalize_text(text: str) -> str: """ Strong normalization to reduce layout-induced CER/WER inflation. Intended for OCR benchmarking, not semantic comparison. """ if not text: return "" text = text.lower() text = re.sub(r"-\n", "", text) # fix hyphenated line breaks text = re.sub(r"\n+", "\n", text) # collapse multiple newlines text = re.sub(r"[^\w\s]", "", text) # remove punctuation text = re.sub(r"\s+", " ", text) # normalize spaces return text.strip() # ----------------------------- # Metrics # ----------------------------- def compute_cer(ground_truth: str, ocr_text: str) -> float: """ Character Error Rate (CER) """ gt = normalize_text(ground_truth) pred = normalize_text(ocr_text) if not gt: return 0.0 return round(cer(gt, pred), 4) def compute_wer(ground_truth: str, ocr_text: str) -> float: """ Word Error Rate (WER) """ gt = normalize_text(ground_truth) pred = normalize_text(ocr_text) if not gt: return 0.0 return round(wer(gt, pred), 4) def character_accuracy(ground_truth: str, ocr_text: str) -> float: """ Character-level accuracy (1 - normalized Levenshtein distance) """ gt = normalize_text(ground_truth) pred = normalize_text(ocr_text) if not gt: return 0.0 dist = Levenshtein.distance(gt, pred) return round(1 - dist / max(len(gt), 1), 4) # ----------------------------- # Unified report # ----------------------------- def accuracy_report(ground_truth: str, ocr_text: str) -> Dict[str, float]: """ Unified OCR accuracy report. """ return { "CER": compute_cer(ground_truth, ocr_text), "WER": compute_wer(ground_truth, ocr_text), "Char_Accuracy": character_accuracy(ground_truth, ocr_text), } def validate_compliance(gt_json: dict, ocr_text: str): results = [] text_lower = ocr_text.lower() required_sections = [ "balance sheet", "statement of profit and loss", "cash flow", "auditor" ] for section in required_sections: results.append({ "rule": f"{section.title()} Present", "status": "PASS" if section in text_lower else "FAIL" }) if "tables" in gt_json: for table in gt_json.get("tables", []): title = str(table.get("title", "")).lower() if title: results.append({ "rule": f"Table Detected: {title}", "status": "PASS" if title in text_lower else "FAIL" }) return results from typing import List, Dict def bbox_iou(boxA, boxB): """ Compute Intersection over Union between two boxes. box format: [x, y, width, height] """ ax1, ay1, aw, ah = boxA bx1, by1, bw, bh = boxB ax2, ay2 = ax1 + aw, ay1 + ah bx2, by2 = bx1 + bw, by1 + bh inter_x1 = max(ax1, bx1) inter_y1 = max(ay1, by1) inter_x2 = min(ax2, bx2) inter_y2 = min(ay2, by2) inter_w = max(0, inter_x2 - inter_x1) inter_h = max(0, inter_y2 - inter_y1) inter_area = inter_w * inter_h areaA = aw * ah areaB = bw * bh union = areaA + areaB - inter_area return inter_area / union if union else 0 def evaluate_layout( gt_blocks: List[Dict], pred_blocks: List[Dict], iou_threshold: float = 0.5 ): """ Match predicted layout blocks to GT using IoU. Evaluate category classification accuracy. """ matched = 0 correct_class = 0 for gt in gt_blocks: best_iou = 0 best_pred = None for pred in pred_blocks: iou = bbox_iou(gt["bbox"], pred["bbox"]) if iou > best_iou: best_iou = iou best_pred = pred if best_iou >= iou_threshold and best_pred: matched += 1 if best_pred["category"] == gt["category"]: correct_class += 1 total = len(gt_blocks) return { "layout_total_blocks": total, "layout_matched_blocks": matched, "layout_classification_accuracy": round(correct_class / total, 4) if total else 0 } import argparse import json from pathlib import Path from engines.docling_engine import OCREngine from metrics.accuracy import accuracy_report from metrics.accuracy_financial_new import financial_accuracy_report from metrics.compliance_rules import validate_compliance def normalize(text: str) -> str: return " ".join(text.lower().split()) def extract_gt_text(gt_json): texts = [] def walk(obj): if isinstance(obj, dict): for v in obj.values(): walk(v) elif isinstance(obj, list): for item in obj: walk(item) elif isinstance(obj, str): t = obj.strip() if t: texts.append(t) walk(gt_json) return "\n".join(texts) def run_benchmark(dataset_dir: Path): engine = OCREngine() pdfs = list(dataset_dir.glob("*.pdf")) if not pdfs: print("‚ùå No PDFs found") return page_limits_file = dataset_dir / "page_limits.json" page_limits = {} if page_limits_file.exists(): with open(page_limits_file, encoding="utf-8") as f: page_limits = json.load(f) total_pages = 0 total_time = 0 cer_sum = 0 wer_sum = 0 char_acc_sum = 0 fin_sum = 0 doc_count = 0 per_doc_results = [] for pdf in pdfs: gt_path = dataset_dir / f"{pdf.stem}_gt.json" if not gt_path.exists(): print(f"‚ö† Skipping {pdf.name} (GT missing)") continue print("\n====================================") print(f"üìÑ {pdf.name}") print("====================================") limits = page_limits.get(pdf.name, {}) start_page = limits.get("start_page") end_page = limits.get("end_page") result = engine.process_pdf( str(pdf), start_page=start_page, end_page=end_page, ) if not result["success"]: print("‚ùå OCR failed") continue with open(gt_path, encoding="utf-8") as f: gt_json = json.load(f) gt_text = extract_gt_text(gt_json) # Technical OCR ocr_metrics = accuracy_report( normalize(gt_text), normalize(result["text"]) ) print("\n--- Technical OCR ---") print(f"CER: {round(ocr_metrics['CER'],4)}") print(f"WER: {round(ocr_metrics['WER'],4)}") print(f"Char Accuracy: {round(ocr_metrics['Char_Accuracy'],4)}") # Financial fin_metrics = financial_accuracy_report( gt_json, result["text"] ) print("\n--- Financial ---") print(f"Numeric Accuracy: {fin_metrics['numeric_accuracy']}") print(f"Line Item Accuracy: {fin_metrics['line_item_accuracy']}") print(f"Section Accuracy: {fin_metrics['section_accuracy']}") print(f"Disclosure Accuracy: {fin_metrics['disclosure_accuracy']}") print(f"\nFinancial Overall Score: {fin_metrics['financial_overall_score']}") # Compliance print("\n--- Compliance ---") compliance_results = validate_compliance(gt_json, result["text"]) for rule in compliance_results: print(f"{rule['rule']}: {rule['status']}") # Aggregate pages = result["pages"] doc_count += 1 total_pages += pages total_time += result["time_sec"] cer_sum += ocr_metrics["CER"] * pages wer_sum += ocr_metrics["WER"] * pages char_acc_sum += ocr_metrics["Char_Accuracy"] * pages fin_sum += fin_metrics["financial_overall_score"] per_doc_results.append({ "pdf": pdf.name, "CER": ocr_metrics["CER"], "WER": ocr_metrics["WER"], "Char_Accuracy": ocr_metrics["Char_Accuracy"], "Financial_Score": fin_metrics["financial_overall_score"] }) if doc_count == 0: print("No documents processed.") return final_summary = { "documents_processed": doc_count, "total_pages": total_pages, "average_CER": round(cer_sum / total_pages, 4), "average_WER": round(wer_sum / total_pages, 4), "average_character_accuracy": round(char_acc_sum / total_pages, 4), "average_financial_score": round(fin_sum / doc_count, 4), "average_time_per_page_sec": round(total_time / total_pages, 4), } print("\n==============================") print("üìä FINAL SUMMARY") print("==============================") print(json.dumps(final_summary, indent=2)) output_path = dataset_dir / "benchmark_full_report.json" output_path.write_text( json.dumps({ "per_document_results": per_doc_results, "final_summary": final_summary }, indent=2), encoding="utf-8" ) print(f"\nüìÅ {output_path.name} saved") if __name__ == "__main__": parser = argparse.ArgumentParser("Financial OCR Benchmark Runner") parser.add_argument("--dataset", required=True) args = parser.parse_args() run_benchmark(Path(args.dataset)) import json import argparse from pathlib import Path from engines.docling_engine import OCREngine from metrics.accuracy import accuracy_report from metrics.accuracy_financial import financial_accuracy_report # -------------------------------- # Engine Registry # -------------------------------- ENGINES = { "docling": OCREngine, } # -------------------------------- # Detect GT Type # -------------------------------- def detect_gt_type(gt_json): if isinstance(gt_json, dict) and "html" in gt_json: return "html" if isinstance(gt_json, dict) and "tables" in gt_json: return "structured" if isinstance(gt_json, list): return "word_level" return "unknown" # -------------------------------- # Extract Word-Level GT # -------------------------------- def extract_word_level_text(gt_json): return " ".join( item["text"] for item in gt_json if isinstance(item, dict) and "text" in item ) # -------------------------------- # Aggregator # -------------------------------- class FinTabNetAggregator: def __init__(self): self.total_pages = 0 self.total_time = 0 self.financial_scores = [] self.cer_scores = [] self.wer_scores = [] self.doc_count = 0 def add(self, pages, time_sec, fin=None, text=None): self.total_pages += pages self.total_time += time_sec self.doc_count += 1 if fin is not None: self.financial_scores.append(fin) if text: self.cer_scores.append(text["CER"]) self.wer_scores.append(text["WER"]) def final(self): avg_fin = sum(self.financial_scores)/len(self.financial_scores) if self.financial_scores else 0 avg_cer = sum(self.cer_scores)/len(self.cer_scores) if self.cer_scores else 0 avg_wer = sum(self.wer_scores)/len(self.wer_scores) if self.wer_scores else 0 avg_time = self.total_time/self.total_pages if self.total_pages else 0 return { "documents_processed": self.doc_count, "total_pages": self.total_pages, "average_financial_score": round(avg_fin,4), "average_CER": round(avg_cer,4), "average_WER": round(avg_wer,4), "average_time_per_page_sec": round(avg_time,4) } # -------------------------------- # MAIN # -------------------------------- def run_fintabnet_benchmark(dataset_dir: Path, engine_name: str): if engine_name not in ENGINES: print("‚ùå Unknown engine") return engine = ENGINES[engine_name]() agg = FinTabNetAggregator() pdfs = list(dataset_dir.glob("*.pdf")) if not pdfs: print("‚ùå No PDFs found") return for pdf in pdfs: gt_path = pdf.with_name(pdf.stem + "_words.json") if not gt_path.exists(): print("‚ö† Missing GT:", pdf.name) continue print("\nüîç", pdf.name) result = engine.process_pdf(str(pdf)) if not result["success"]: print("‚ùå OCR failed") continue with open(gt_path, encoding="utf-8") as f: gt_json = json.load(f) gt_type = detect_gt_type(gt_json) # ===================================================== # WORD LEVEL DATASET # ===================================================== if gt_type == "word_level": gt_text = extract_word_level_text(gt_json) pred = result.get("text","") text_metrics = accuracy_report(gt_text.lower(), pred.lower()) fin_metrics = financial_accuracy_report(gt_json, pred) agg.add( result.get("pages",1), result.get("time_sec",0), fin=fin_metrics["financial_overall_score"], text=text_metrics ) print("Mode:", gt_type) print("CER:", round(text_metrics["CER"],4)) print("WER:", round(text_metrics["WER"],4)) print("Financial Score:", fin_metrics["financial_overall_score"]) # ===================================================== # STRUCTURED / HTML DATASET # ===================================================== else: metrics = financial_accuracy_report( gt_json, result.get("markdown","") ) agg.add( result.get("pages",1), result.get("time_sec",0), fin=metrics["financial_overall_score"] ) print("Mode:", metrics["mode"]) print("Score:", metrics["financial_overall_score"]) final = agg.final() print("\n==============================") print("üìä FINAL RESULTS") print("==============================") for k,v in final.items(): print(k,":",v) out = f"fintabnet_benchmark_summary_{engine_name}.json" Path(out).write_text(json.dumps(final,indent=2)) print("\nSaved ‚Üí", out) # -------------------------------- # CLI # -------------------------------- if __name__ == "__main__": parser = argparse.ArgumentParser() parser.add_argument("--dataset", required=True) parser.add_argument("--engine", default="docling", choices=ENGINES.keys()) args = parser.parse_args() run_fintabnet_benchmark(Path(args.dataset), args.engine) import json import argparse from pathlib import Path from typing import Dict from metrics.accuracy import accuracy_report # ----------------------------- # Engine Registry # ----------------------------- ENGINES = { "docling": "engines.docling_engine.OCREngine", "hybrid": "engines.hybrid_engine.HybridOCREngine", "native": "engines.docling_native_engine.DoclingNativeEngine", } # ----------------------------- # Extract GT text (DocLayNet format) # ----------------------------- def extract_doclaynet_text(gt_json: dict) -> str: texts = [] if "form" in gt_json: for block in gt_json["form"]: if isinstance(block, dict) and "text" in block: texts.append(block["text"]) if "lines" in gt_json: for line in gt_json["lines"]: texts.append(line.get("text", "")) return "\n".join(texts) def normalize(text: str) -> str: return " ".join(text.lower().split()) # ----------------------------- # Aggregator # ----------------------------- class BenchmarkAggregator: def __init__(self): self.total_pages = 0 self.total_time = 0 self.cer_sum = 0 self.wer_sum = 0 self.char_acc_sum = 0 self.doc_count = 0 def add(self, pages: int, time_sec: float, metrics: Dict): self.total_pages += pages self.total_time += time_sec self.doc_count += 1 self.cer_sum += metrics["CER"] * pages self.wer_sum += metrics["WER"] * pages self.char_acc_sum += metrics["Char_Accuracy"] * pages def final(self): if self.total_pages == 0: return {} return { "documents_processed": self.doc_count, "total_pages": self.total_pages, "average_CER": round(self.cer_sum / self.total_pages, 4), "average_WER": round(self.wer_sum / self.total_pages, 4), "average_character_accuracy": round(self.char_acc_sum / self.total_pages, 4), "average_time_per_page_sec": round(self.total_time / self.total_pages, 4), } # ----------------------------- # MAIN # ----------------------------- def run_doclaynet_benchmark(dataset_dir: Path, engine_name: str): if engine_name not in ENGINES: print(f"‚ùå Unknown engine: {engine_name}") return import importlib module_path, class_name = ENGINES[engine_name].rsplit(".", 1) module = importlib.import_module(module_path) engine_class = getattr(module, class_name) engine = engine_class() aggregator = BenchmarkAggregator() pdf_files = list(dataset_dir.glob("*.pdf")) if not pdf_files: print("‚ùå No PDFs found in dataset folder") return for pdf_path in pdf_files: gt_path = pdf_path.with_suffix(".json") if not gt_path.exists(): print(f"‚ö† Skipping {pdf_path.name} ‚Äî No GT JSON found") continue print(f"\nüîç Processing: {pdf_path.name}") result = engine.process_pdf(str(pdf_path)) if not result["success"]: print(f"‚ùå OCR failed: {pdf_path.name}") continue with open(gt_path, encoding="utf-8") as f: gt_json = json.load(f) gt_text = extract_doclaynet_text(gt_json) pred_text = result["text"] if not gt_text.strip(): print(f"‚ö† No GT text found in {gt_path.name}") continue metrics = accuracy_report( normalize(gt_text), normalize(pred_text) ) aggregator.add( pages=result["pages"], time_sec=result["time_sec"], metrics=metrics ) print("CER:", round(metrics["CER"], 4)) print("WER:", round(metrics["WER"], 4)) final = aggregator.final() print("\n==============================") print(f"üìä DOCLAYNET BENCHMARK RESULTS ({engine_name.upper()})") print("==============================") for k, v in final.items(): print(f"{k}: {v}") output_file = f"doclaynet_benchmark_summary_{engine_name}.json" Path(output_file).write_text( json.dumps(final, indent=2), encoding="utf-8" ) print(f"\nüìÅ {output_file} saved") # ----------------------------- # CLI Entry # ----------------------------- if __name__ == "__main__": parser = argparse.ArgumentParser() parser.add_argument( "--dataset", default=r"C:\Users\dewan\AI-FRC\data\benchmark\DocLayNet\test\pdfs", help="Path to dataset folder" ) parser.add_argument( "--engine", default="docling", choices=ENGINES.keys(), help="OCR engine to use" ) args = parser.parse_args() run_doclaynet_benchmark( dataset_dir=Path(args.dataset), engine_name=args.engine )"""